---
layout: post
title: Overview of Schedulers
---

A gentle guide to Schedulers, in plain English
----

## Fourth Improvement to Gradient Descent - Modify Learning Rate (based on your training progress)
This topic is actually not under Optimizers at all. This is handled by Schedulers. They vary the learning rate and other optimization hyperparameters based on a formula that depends on the epoch of the training.

## Conclusion


