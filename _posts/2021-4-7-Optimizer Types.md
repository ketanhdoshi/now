---
layout: post
title: Popular Optimizer Types
---

A gentle guide to Popular Optimizer Types, in plain English
----

## Details of Optimizers - Momentum with Exponential Moving Avg, RMSProp and Adam
Include some of my Pytorch code from optimiser_lib to show the Python Step code for calculating each optimiser.

Show formula for each optimizer

Detailed explanation of moving averages and how momentum works.

RMSProp adjusts learning rate separately for each parameter. And so the formula applies to the projection ie. the component of the gradient that lies along the direction/axis of the parameter we're updating.

## Conclusion


